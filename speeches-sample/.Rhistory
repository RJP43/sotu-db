install.packages("tidytext")
library(dplyr)
text_df <- data_frame(line = 1:4, text = text)
text_df <- data_frame(line = 1:4, text = text)
load("C:/Users/tnmon/Desktop/sotu-db/speeches - gutenberg/2016-Obama.txt")
load("~/LUC/SOTU-DB/speeches - gutenberg/1965-Johnson.txt")
library(dplyr)
text_df <- data_frame(line = 1:4, text = text)
text_df
text_df <- data_frame(line = 1:4, text = text)
text_df <- tibble(1:5)
View(text_df)
install.packages("formatR", repos = "http://cran.us.r-project.org")
install.packages(c("tidyverse", "tidytext", "glue", "stringr", "tm"))
install.packages(c("tidyverse", "tidytext", "glue", "stringr", "tm"))
initCoreNLP()
#### debug defaults ####
textString = "If we work together, we can make some big farts. Together, anything is possible, together."
userQuery = toString("together")
userQuery=toString("Farts")
chunkSize = 65
#annotationByCoreNLP = annotateFile("../speeches-sample/2013-02-12-obama.md")
annotationByCoreNLP = annotateString(textString)
coreNLPtokens = getToken(annotationByCoreNLP)
sentiment = getSentiment(annotationByCoreNLP)
matches = coreNLPtokens %>%
str_match_all(userQuery)
#### str_count(x,pattern) ####
#case sensitive:
str_count(textString, userQuery)
#case insensitive:
regexUserQuery = paste0("(?i)",userQuery)
regexUserQuerySentence = paste0("[^.]*",regexUserQuery,"[^.]*\\.")
str_count(textString, regexUserQuery)
stringCountResult = str_count(textString, regexUserQuery)
#### str_subset(x,pattern) ####
str_subset(textString, "")
WTF
#### str_locate(x,pattern) ####
stringLocateResult = str_locate_all(textString, regexUserQuery)
#### str_extract(x,pattern) ####
stringExtractResults=str_extract_all(textString,regexUserQuery)
stringExtractResultSimplified = str_extract_all(textString,regexUserQuery, simplify=TRUE)
#### str_match_all(x,pattern) ####
stringMatchResult= str_match_all(textString, regexUserQuerySentence)
library(tidyverse)
library(tidytext)
library(glue)
library(stringr)
library(tm)
initCoreNLP()
#### debug defaults ####
textString = "If we work together, we can make some big farts. Together, anything is possible, together."
userQuery = toString("together")
userQuery=toString("Farts")
chunkSize = 65
#annotationByCoreNLP = annotateFile("../speeches-sample/2013-02-12-obama.md")
annotationByCoreNLP = annotateString(textString)
coreNLPtokens = getToken(annotationByCoreNLP)
sentiment = getSentiment(annotationByCoreNLP)
matches = coreNLPtokens %>%
str_match_all(userQuery)
#### str_count(x,pattern) ####
#case sensitive:
str_count(textString, userQuery)
#case insensitive:
regexUserQuery = paste0("(?i)",userQuery)
regexUserQuerySentence = paste0("[^.]*",regexUserQuery,"[^.]*\\.")
str_count(textString, regexUserQuery)
stringCountResult = str_count(textString, regexUserQuery)
#### str_subset(x,pattern) ####
str_subset(textString, "")
WTF
#### str_locate(x,pattern) ####
stringLocateResult = str_locate_all(textString, regexUserQuery)
#### str_extract(x,pattern) ####
stringExtractResults=str_extract_all(textString,regexUserQuery)
stringExtractResultSimplified = str_extract_all(textString,regexUserQuery, simplify=TRUE)
#### str_match_all(x,pattern) ####
stringMatchResult= str_match_all(textString, regexUserQuerySentence)
#load("C:/Users/tnmon/git/sotu-db/r-env/tidytokens-sample.RData")
#load("/var/www/sotu-db.cs.luc.edu/html/r-env/tidytokens-sample.RData")
load("../r-env/tidytokens-sample.RData")
setwd("C:/Users/tnmon/git/sotu-db/speeches-sample")
load("../r-env/tidytokens-sample.RData")
yearSearched=2001
singleSOTU <- tidytokens %>% filter(year == yearSearched)
annotationByCoreNLP = annotateFile("../speeches-sample/2013-02-12-obama.md")
#### invoke coreNLP pipeline: pos, sentiment ####
initCoreNLP()
library(coreNLP)
#### invoke coreNLP pipeline: pos, sentiment ####
initCoreNLP()
userQuery=toString("Farts")
chunkSize = 65
yearSearched=2001
singleSOTU <- tidytokens %>% filter(year == yearSearched)
yearSearched=2013
singleSOTU <- tidytokens %>% filter(year == yearSearched)
annotationByCoreNLP = annotateFile("../speeches-sample/2013-02-12-obama.md")
coreNLPtokens = getToken(annotationByCoreNLP)
sentiment = getSentiment(annotationByCoreNLP)
matches = coreNLPtokens %>%
str_match_all(userQuery)
View(matches)
regexUserQuery = paste0("(?i)",userQuery)
regexUserQuerySentence = paste0("[^.]*",regexUserQuery,"[^.]*\\.")
coreNLPtokens = getToken(annotationByCoreNLP)
sentiment = getSentiment(annotationByCoreNLP)
matches = coreNLPtokens %>%
str_match_all(userQuery)
View(matches)
str_count(textString, userQuery)
#case insensitive:
str_count(textString, regexUserQuery)
stringCountResult = str_count(textString, regexUserQuery)
#### str_subset(x,pattern) ####
str_subset(textString, "")
WTF
#### str_locate(x,pattern) ####
stringLocateResult = str_locate_all(textString, regexUserQuery)
#### str_extract(x,pattern) ####
stringExtractResults=str_extract_all(textString,regexUserQuery)
stringExtractResultSimplified = str_extract_all(textString,regexUserQuery, simplify=TRUE)
#### str_match_all(x,pattern) ####
stringMatchResult= str_match_all(textString, regexUserQuerySentence)
str_match_all(,regexUserQuerySentence)
userQuery = toString("together")
chunkSize = 65
yearSearched=2013
singleSOTU <- tidytokens %>% filter(year == yearSearched)
regexUserQuery = paste0("(?i)",userQuery)
regexUserQuerySentence = paste0("[^.]*",regexUserQuery,"[^.]*\\.")
annotationByCoreNLP = annotateFile("../speeches-sample/2013-02-12-obama.md")
userQuery = toString("together")
chunkSize = 65
yearSearched=2013
singleSOTU <- tidytokens %>% filter(year == yearSearched)
regexUserQuery = paste0("(?i)",userQuery)
regexUserQuerySentence = paste0("[^.]*",regexUserQuery,"[^.]*\\.")
coreNLPtokens = getToken(annotationByCoreNLP)
sentiment = getSentiment(annotationByCoreNLP)
userQuery = toString("together")
chunkSize = 65
yearSearched=2013
singleSOTU <- tidytokens %>% filter(year == yearSearched)
regexUserQuery = paste0("(?i)",userQuery)
regexUserQuerySentence = paste0("[^.]*",regexUserQuery,"[^.]*\\.")
coreNLPtokens = getToken(annotationByCoreNLP)
sentiment = getSentiment(annotationByCoreNLP)
userQuery = toString("together")
chunkSize = 65
yearSearched=2013
singleSOTU <- tidytokens %>% filter(year == yearSearched)
regexUserQuery = paste0("(?i)",userQuery)
regexUserQuerySentence = paste0("[^.]*",regexUserQuery,"[^.]*\\.")
coreNLPtokens = getToken(annotationByCoreNLP)
sentiment = getSentiment(annotationByCoreNLP)
#### ? ####
matches = coreNLPtokens %>%
str_match_all(userQuery)
#### str_count(x,pattern) ####
#case sensitive:
str_count(textString, userQuery)
#case insensitive:
str_count(textString, regexUserQuery)
str_count(singleSOTU, userQuery)
str_count(singleSOTU, regexUserQuery)
str_count(singleSOTU[1], regexUserQuery)
textSOTU = writeLines(as.character(singleSOTU[[1]]))
textSOTU = writeLines(as.character("../speeches-sample/2013-02-12-obama.md"))
textSOTU = toString("../speeches-sample/2013-02-12-obama.md"))
textSOTU = toString("../speeches-sample/2013-02-12-obama.md")
textSOTU = toString(FILE="../speeches-sample/2013-02-12-obama.md")
textSOTU = writeLines(file("../speeches-sample/2013-02-12-obama.md"))
file = file("../speeches-sample/2013-02-12-obama.md")
textSOTU = writeLines(file)
fileConn = file("../speeches-sample/2013-02-12-obama.md")
textSOTU = writeLines(fileConn)
close(fileConn)
fileConn = file("../speeches-sample/2013-02-12-obama.md")
textSOTU = writeLines(toString(fileConn))
close(fileConn)
fileConn = file("../speeches-sample/2013-02-12-obama.md")
textSOTU = writeLines(as.character(fileConn))
textSOTU = writeLines(file("../speeches-sample/2013-02-12-obama.md"))
textSOTU = as.character(file("../speeches-sample/2013-02-12-obama.md"))
textSOTU
textSOTU = toString(file("../speeches-sample/2013-02-12-obama.md"))
textSOTU = toString(file("../speeches-sample/2013-02-12-obama.md"))
lines = readLines("../speeches-sample/2013-02-12-obama.md")
plainTextSOTU = readLines("../speeches-sample/2013-02-12-obama.md")
library(tidyverse)
library(tidytext)
library(glue)
library(stringr)
library(tm)
library(coreNLP)
load("../r-env/tidytokens-sample.RData")
plainTextSOTU = readLines("../speeches-sample/2013-02-12-obama.md")
#annotationByCoreNLP = annotateString(textString)
#### invoke coreNLP pipeline: pos, sentiment ####
initCoreNLP()
userQuery = toString("together")
chunkSize = 65
#yearSearched=2013
#singleSOTU <- tidytokens %>% filter(year == yearSearched)
regexUserQuery = paste0("(?i)",userQuery)
regexUserQuerySentence = paste0("[^.]*",regexUserQuery,"[^.]*\\.")
#### ? ####
matches = coreNLPtokens %>%
str_match_all(userQuery)
#### str_count(x,pattern) ####
#case sensitive:
str_count(plainTextSOTU, userQuery)
#### str_count(x,pattern) ####
#case sensitive:
str_count(plainTextSOTU, userQuery)
#case insensitive:
str_count(plainTextSOTU, regexUserQuery)
stringCountResult = str_count(plainTextSOTU, regexUserQuery)
#### str_locate(x,pattern) ####
stringLocateResult = str_locate_all(plainTextSOTU, regexUserQuery)
str_locate_all(plainTextSOTU, regexUserQuery)
#### str_extract(x,pattern) ####
stringExtractResults=str_extract_all(plainTextSOTU,regexUserQuery)
stringExtractResultSimplified = str_extract_all(plainTextSOTU,regexUserQuery, simplify=TRUE)
#### str_match_all(x,pattern) ####
stringMatchResult= str_match_all(plainTextSOTU, regexUserQuerySentence)
View(stringMatchResult)
#load("C:/Users/tnmon/git/sotu-db/r-env/tidytokens-sample.RData")
#load("/var/www/sotu-db.cs.luc.edu/html/r-env/tidytokens-sample.RData")
load("../r-env/tidytokens-sample.RData")
library(tidyverse)
library(tidytext)
library(glue)
library(stringr)
library(tm)
library(dplyr)
files <- list.files("./") # get a list of the files in the input directory
#files <- SOTUs4 # get a list of the documents in SOTUs4
#toDollars = content_transformer(function(x,pattern) {return (gsub(pattern, "dollars ", x))}) #use toDollars to change dollar signs to "dollars"
tidytokens = data_frame() #make the tidytokens data_frame (empty for now)
GetTidy = function(file) {
fileName <- glue("./", file, sep = "")
fileName <- trimws(fileName)   # get rid of any whitespace
fileText <- glue(read_file(fileName))  # read in the new file
#fileText <- read_file(file)  # read in the new file
# tokenize
tokens <- data_frame(text = fileText) %>% unnest_tokens(word, text, to_lower=FALSE)
tidytokens <- tokens %>%
#group_by(president) %>%
mutate(file = file) %>% # add the name of our file
mutate(year = as.numeric(str_match(file, "\\d{4}"))) %>% # add the year
mutate(linenumber = row_number()) %>% #line number currently is just one "line" per token but at least it gives its position within the SOTU
# REGEX for yyyy-Potusname-m-d.txt, uncomment to use:
# mutate(president = str_match(file, "(?<=-)[A-z]+(?=-)")) %>% #thanks @RJP43 for helping with this REGEX
# REGEX for mm-dd-yyyy-potusname.md
mutate(president = str_match(file, "(?<=-)[A-z]+(?=.md)")) #%>%
#never figured out how to do this without encoding directly in the filename.
#ultimately i want to be able to read from the master index file.
#perhaps reading that file into R and writing it as variables, then performing these tasks with those variables would work.
#mutate(party = str_match(file, "[A-z](?=[.])"))
# return our tidytokens dataframe
return(tidytokens)
}
for(i in files){
tidytokens = rbind(tidytokens, GetTidy(i))
}
View(tidytokens)
#create corpus
SOTUs = Corpus(DirSource("./"))
save.image("C:/Users/tnmon/git/sotu-db/r-env/tidytokens-sample.RData")
library(tidyverse)
library(tidytext)
library(glue)
library(stringr)
library(tm)
library(coreNLP)
load("../r-env/tidytokens-sample.RData")
plainTextSOTU = readLines("../speeches-sample/2013-02-12-obama.md")
#### invoke coreNLP pipeline: pos, sentiment ####
initCoreNLP()
#### debug defaults ####
userQuery = toString("together")
chunkSize = 65
regexUserQuery = paste0("(?i)",userQuery)
regexUserQuerySentence = paste0("[^.]*",regexUserQuery,"[^.]*\\.")
#### str_count(x,pattern) ####
#case sensitive:
str_count(plainTextSOTU, userQuery)
#case insensitive:
stringCountResult = str_count(plainTextSOTU, regexUserQuery)
sum = sum(stringCountResult)
sum
string = "what the fuck man"
plainTextSOTU
#### load packages  and workspace ####
library(tidyverse)
library(tidytext)
library(glue)
library(stringr)
library(tm)
library(coreNLP)
load("../r-env/tidytokens-sample.RData")
#annotationByCoreNLP = annotateFile("../speeches-sample/2013-02-12-obama.md")
plainTextSOTU = readLines("../speeches-sample/2013-02-12-obama.md")
#annotationByCoreNLP = annotateString(plainTextSOTU)
#### invoke coreNLP pipeline: pos, sentiment ####
initCoreNLP()
#### debug defaults ####
userQuery = toString("together")
chunkSize = 65
#yearSearched=2013
#singleSOTU <- tidytokens %>% filter(year == yearSearched)
regexUserQuery = paste0("(?i)",userQuery)
regexUserQuerySentence = paste0("[^.]*",regexUserQuery,"[^.]*\\.")
#coreNLPtokens = getToken(annotationByCoreNLP)
#sentiment = getSentiment(annotationByCoreNLP)
#### ? ####
#matches = coreNLPtokens %>%
#  str_match_all(userQuery)
#### str_count(x,pattern) ####
#case sensitive:
#str_count(plainTextSOTU, userQuery)
#case insensitive:
stringCountResult = str_count(plainTextSOTU, regexUserQuery)
stringCountSum = sum(stringCountResult)
#### str_locate(x,pattern) ####
stringLocateResult = str_locate_all(plainTextSOTU, regexUserQuery)
View(stringLocateResult)
#### str_extract(x,pattern) ####
stringExtractResults=str_extract_all(plainTextSOTU,regexUserQuery)
stringExtractResultSimplified = str_extract_all(plainTextSOTU,regexUserQuery, simplify=TRUE)
View(stringLocateResult)
View(stringExtractResultSimplified)
#### str_match_all(x,pattern) ####
#this creates a list of 185, and the populated cells are the sentences with the users search word in them.
stringMatchResult= str_match_all(plainTextSOTU, regexUserQuerySentence)
View(stringMatchResult)
lapply(stringMatchResult, write, "test.txt", append=FALSE)
lapply(stringMatchResult, write, "test.txt", append=TRUE)
writeLines(unlist(lapply(stringMatchResult, paste, collapse=" ")))
trimws
writeLines(trimws(unlist(lapply(stringMatchResult, paste, collapse=" "))))
writeLines(unlist(lapply(stringMatchResult, paste, collapse=" ")))
stringMatchResult[lapply(stringMatchResult,length)>0]
stringMatchResult = stringMatchResult[lapply(stringMatchResult,length)>0]
lapply(stringMatchResult, write, "test.txt", append=TRUE)
writeLines(unlist(lapply(stringMatchResult, paste, collapse=" ")))
lapply(stringMatchResult, write, "test.txt", append=FALSE)
chunkSize = as.integer(args[2])
rownames(stringMatchResult) = c(1,2,3,4,5,6,7,8,9,10)
rownames(stringMatchResult) = c("1","2","3","4","5","6","7","8","9","10")
stringMatchResult= cbind(1, 1:4)
View(stringMatchResult)
stringMatchResult= str_match_all(plainTextSOTU, regexUserQuerySentence)
stringMatchResult = stringMatchResult[lapply(stringMatchResult,length)>0]
stringMatchResultMatrix= cbind(stringMatchResult, 1:stringMatchResult.length)
stringMatchResultMatrix= cbind(stringMatchResult, 1:length(stringMatchResult))
View(stringMatchResultMatrix)
stringMatchResultMatrix= rownames(stringMatchResult, 1:length(stringMatchResult))
#### silence output ####
#linux
#sink("/dev/null")
#windows
sink("../output/log.txt", append=FALSE, type=c("output","message"))
#### silence output ####
#linux
#sink("/dev/null")
#windows
tempPath=file("log.txt",open="w")
sink(tempPath, type=c("message","output"))
#### silence output ####
#linux
#sink("/dev/null")
#windows
tempPath=file("log.txt",open="w")
sink(tempPath, type=c("message","output"))
sink(tempPath, type="message")
#### str_count(x,pattern) ####
#case sensitive:
#str_count(plainTextSOTU, userQuery)
sink()
sink("../output/log.txt", append=FALSE, type=c("output","message"))
#### str_count(x,pattern) ####
#case sensitive:
#str_count(plainTextSOTU, userQuery)
sink(NULL)
#### str_count(x,pattern) ####
#case sensitive:
#str_count(plainTextSOTU, userQuery)
sink()
sink()
